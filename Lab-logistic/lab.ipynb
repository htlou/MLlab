{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备工作\n",
    "### 环境准备\n",
    "\n",
    "请确保完成以下依赖包的安装，并且通过下面代码来导入与验证。运行成功后，你会看到一个新的窗口，其展示了一张空白的figure。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "\n",
    "# display the plot in a separate window\n",
    "%matplotlib tk\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "# create a figure and axis\n",
    "plt.ion()\n",
    "fig = plt.figure(figsize=(12, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集准备\n",
    "\n",
    "你将使用以下二维数据集来训练逻辑分类器，并观察随着训练的进行，线性分割面的变化。\n",
    "\n",
    "该数据集包含两个特征和一个标签，其中标签 $ y \\in \\{-1,1\\} $。\n",
    "\n",
    "请执行下面的代码以加载数据集并对其进行可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generator import gen_2D_dataset\n",
    "\n",
    "x_train, y_train = gen_2D_dataset(100, 100, noise = 0)\n",
    "x_test, y_test = gen_2D_dataset(50, 50, noise = 0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis_util import visualize_2D_dataset, visualize_2D_border\n",
    "\n",
    "visualize_2D_dataset(x_train, y_train)\n",
    "visualize_2D_dataset(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归 (10 pts)\n",
    "\n",
    "在这一部分，你将学习并完成逻辑回归相关代码的编写与训练。\n",
    "\n",
    "在运行这部分代码之前，请确保你已经完成了 `logistics.py` 文件的代码补全。\n",
    "\n",
    "完成后，运行以下代码，你会看到一张figure来展示$||w||$，loss和决策边界的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 0.6807266354549091, w_module: 1.863605190916175\n",
      "iter: 10, loss: 0.6047988783795631, w_module: 1.8463015287291755\n",
      "iter: 20, loss: 0.5660832091387538, w_module: 1.8229347764809727\n",
      "iter: 30, loss: 0.5309314279730247, w_module: 1.8003667428545462\n",
      "iter: 40, loss: 0.49873990315881916, w_module: 1.7800162646162347\n",
      "iter: 50, loss: 0.4692664495478232, w_module: 1.7620409406227315\n",
      "iter: 60, loss: 0.4422816596337917, w_module: 1.7463734583242816\n",
      "iter: 70, loss: 0.4175668558824793, w_module: 1.732898794089459\n",
      "iter: 80, loss: 0.3949164282672946, w_module: 1.7214872221452302\n",
      "iter: 90, loss: 0.3741392267477511, w_module: 1.712003083354751\n",
      "iter: 100, loss: 0.35505915516223857, w_module: 1.7043089950428967\n",
      "iter: 110, loss: 0.33751516416979, w_module: 1.698268763393051\n",
      "iter: 120, loss: 0.32136081353797324, w_module: 1.6937495784510823\n",
      "iter: 130, loss: 0.30646354486334976, w_module: 1.6906236420774396\n",
      "iter: 140, loss: 0.29270377665622754, w_module: 1.6887693160021564\n",
      "iter: 150, loss: 0.27997390696010677, w_module: 1.6880718688190757\n",
      "iter: 160, loss: 0.2681772856991091, w_module: 1.68842389680348\n",
      "iter: 170, loss: 0.2572272001987447, w_module: 1.6897254871507394\n",
      "iter: 180, loss: 0.24704590264325804, w_module: 1.6918841839933128\n",
      "iter: 190, loss: 0.23756369713488554, w_module: 1.6948148085187797\n",
      "iter: 200, loss: 0.2287180959040338, w_module: 1.698439175589564\n",
      "iter: 210, loss: 0.2204530484793579, w_module: 1.7026857410192766\n",
      "iter: 220, loss: 0.21271824371709563, w_module: 1.7074892063821665\n",
      "iter: 230, loss: 0.20546848204891383, w_module: 1.7127901020236742\n",
      "iter: 240, loss: 0.19866311376522536, w_module: 1.7185343637853272\n",
      "iter: 250, loss: 0.19226553831674803, w_module: 1.724672914772374\n",
      "iter: 260, loss: 0.1862427592713795, w_module: 1.7311612601609208\n",
      "iter: 270, loss: 0.1805649895436243, w_module: 1.7379591004363408\n",
      "iter: 280, loss: 0.17520530170196286, w_module: 1.7450299664530993\n",
      "iter: 290, loss: 0.17013931847203295, w_module: 1.7523408781956096\n",
      "iter: 300, loss: 0.16534493893221425, w_module: 1.7598620280021149\n",
      "iter: 310, loss: 0.16080209630385894, w_module: 1.7675664882056246\n",
      "iter: 320, loss: 0.15649254364519968, w_module: 1.7754299425786861\n",
      "iter: 330, loss: 0.1523996641497105, w_module: 1.7834304405861159\n",
      "iter: 340, loss: 0.1485083031168994, w_module: 1.7915481732067742\n",
      "iter: 350, loss: 0.14480461900122626, w_module: 1.7997652689464805\n",
      "iter: 360, loss: 0.14127595125119785, w_module: 1.8080656086013922\n",
      "iter: 370, loss: 0.13791070292575736, w_module: 1.8164346573231311\n",
      "iter: 380, loss: 0.13469823632016772, w_module: 1.824859312567091\n",
      "iter: 390, loss: 0.13162878005067502, w_module: 1.8333277665613013\n",
      "iter: 400, loss: 0.12869334623868037, w_module: 1.8418293820055387\n",
      "iter: 410, loss: 0.12588365660340467, w_module: 1.8503545797922505\n",
      "iter: 420, loss: 0.12319207641954595, w_module: 1.8588947376271638\n",
      "iter: 430, loss: 0.12061155542551866, w_module: 1.8674420985147213\n",
      "iter: 440, loss: 0.11813557488069022, w_module: 1.8759896881591254\n",
      "iter: 450, loss: 0.11575810006855622, w_module: 1.8845312404141688\n",
      "iter: 460, loss: 0.11347353762879621, w_module: 1.8930611299930866\n",
      "iter: 470, loss: 0.11127669717620332, w_module: 1.9015743117227524\n",
      "iter: 480, loss: 0.10916275672998425, w_module: 1.9100662656944047\n",
      "iter: 490, loss: 0.10712723153411044, w_module: 1.918532947725599\n",
      "iter: 500, loss: 0.10516594589934522, w_module: 1.926970744605368\n",
      "iter: 510, loss: 0.10327500774121728, w_module: 1.9353764336468462\n",
      "iter: 520, loss: 0.10145078552638004, w_module: 1.943747146119073\n",
      "iter: 530, loss: 0.09968988737320025, w_module: 1.9520803341727428\n",
      "iter: 540, loss: 0.09798914208168316, w_module: 1.9603737409135216\n",
      "iter: 550, loss: 0.09634558189349451, w_module: 1.9686253733116446\n",
      "iter: 560, loss: 0.09475642680536119, w_module: 1.9768334776680838\n",
      "iter: 570, loss: 0.093219070278911, w_module: 1.98499651738597\n",
      "iter: 580, loss: 0.0917310662074153, w_module: 1.9931131528214947\n",
      "iter: 590, loss: 0.09029011701521598, w_module: 2.0011822230114\n",
      "iter: 600, loss: 0.08889406277912251, w_module: 2.0092027290947536\n",
      "iter: 610, loss: 0.08754087127298131, w_module: 2.0171738192651163\n",
      "iter: 620, loss: 0.08622862884714927, w_module: 2.0250947751057486\n",
      "iter: 630, loss: 0.08495553206391399, w_module: 2.0329649991753245\n",
      "iter: 640, loss: 0.08371988001814948, w_module: 2.0407840037248977\n",
      "iter: 650, loss: 0.0825200672798088, w_module: 2.0485514004387784\n",
      "iter: 660, loss: 0.08135457740134236, w_module: 2.05626689110265\n",
      "iter: 670, loss: 0.08022197693889695, w_module: 2.063930259111829\n",
      "iter: 680, loss: 0.07912090994128188, w_module: 2.0715413617411693\n",
      "iter: 690, loss: 0.07805009286525623, w_module: 2.0791001231057935\n",
      "iter: 700, loss: 0.07700830987976581, w_module: 2.0866065277487746\n",
      "iter: 710, loss: 0.07599440852539532, w_module: 2.094060614798081\n",
      "iter: 720, loss: 0.0750072956985513, w_module: 2.101462472640685\n",
      "iter: 730, loss: 0.07404593393279954, w_module: 2.108812234066741\n",
      "iter: 740, loss: 0.07310933795238438, w_module: 2.1161100718412555\n",
      "iter: 750, loss: 0.07219657147529357, w_module: 2.1233561946647046\n",
      "iter: 760, loss: 0.07130674424532708, w_module: 2.1305508434877285\n",
      "iter: 770, loss: 0.07043900927451287, w_module: 2.137694288148294\n",
      "iter: 780, loss: 0.06959256027890548, w_module: 2.1447868243026873\n",
      "iter: 790, loss: 0.0687666292923296, w_module: 2.1518287706243715\n",
      "iter: 800, loss: 0.06796048444400458, w_module: 2.1588204662471275\n",
      "iter: 810, loss: 0.06717342788722713, w_module: 2.1657622684311026\n",
      "iter: 820, loss: 0.06640479386740848, w_module: 2.172654550432312\n",
      "iter: 830, loss: 0.06565394691877607, w_module: 2.179497699557943\n",
      "iter: 840, loss: 0.06492028017996578, w_module: 2.1862921153913994\n",
      "iter: 850, loss: 0.0642032138195593, w_module: 2.193038208172459\n",
      "iter: 860, loss: 0.06350219356337615, w_module: 2.1997363973192563\n",
      "iter: 870, loss: 0.06281668931600995, w_module: 2.2063871100799695\n",
      "iter: 880, loss: 0.062146193869720874, w_module: 2.212990780303156\n",
      "iter: 890, loss: 0.06149022169435736, w_module: 2.219547847316696\n",
      "iter: 900, loss: 0.060848307802495664, w_module: 2.2260587549061346\n",
      "iter: 910, loss: 0.06022000668445107, w_module: 2.2325239503840653\n",
      "iter: 920, loss: 0.059604891308240295, w_module: 2.2389438837428877\n",
      "iter: 930, loss: 0.059002552179964286, w_module: 2.245319006883957\n",
      "iter: 940, loss: 0.05841259646043255, w_module: 2.2516497729167386\n",
      "iter: 950, loss: 0.05783464713417729, w_module: 2.2579366355221278\n",
      "iter: 960, loss: 0.05726834222729812, w_module: 2.2641800483745893\n",
      "iter: 970, loss: 0.05671333407085248, w_module: 2.270380464618234\n",
      "iter: 980, loss: 0.056169288606752674, w_module: 2.2765383363923566\n",
      "iter: 990, loss: 0.05563588473335918, w_module: 2.2826541144023333\n",
      "[-1.06802541 -2.02356793  5.69720231]\n"
     ]
    }
   ],
   "source": [
    "from logistic import LogisticRegression\n",
    "\n",
    "# create a LogisticRegression object \n",
    "LR = LogisticRegression()\n",
    "\n",
    "# fit the model to the training data without regularization (reg = 0)\n",
    "LR.fit(x_train, y_train, lr=0.1, n_iter=1000,reg=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上述代码，你会发现，在不考虑正则化的情况下，$||w||$ 随着训练次数的增加会不断增大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练完成后，你可以利用训练得到的分类器来进行预测。请你编写代码，计算训练集和测试集中的预测准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 100.0\n",
      "Test accuracy: 98.0\n"
     ]
    }
   ],
   "source": [
    "# Implement the code to compute the accuracy of logistic regression (LR) in the test set. Note that LR itself is already trained, if you have run the above code.\n",
    "\n",
    "# training accuracy\n",
    "\n",
    "# TODO: compute the y_pred using LR.predict() function\n",
    "_, train_pred = LR.predict(x_train)\n",
    "\n",
    "# TODO: compute the accuracy\n",
    "train_acc = np.mean(train_pred == y_train) * 100\n",
    "\n",
    "\n",
    "print(\"Train accuracy: {}\".format(train_acc))\n",
    "\n",
    "\n",
    "# TODO: test accuracy, proceed similarly as above\n",
    "_, test_pred = LR.predict(x_test)\n",
    "test_acc = np.mean(test_pred == y_test) * 100\n",
    "\n",
    "\n",
    "print(\"Test accuracy: {}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 1.141181588625334, w_module: 0.3904283290747596\n",
      "iter: 10, loss: 0.8595507393239986, w_module: 0.07406965014180235\n",
      "iter: 20, loss: 0.7879284649495942, w_module: 0.06578661868177142\n",
      "iter: 30, loss: 0.7326919494437688, w_module: 0.11440695528050666\n",
      "iter: 40, loss: 0.6820895786646239, w_module: 0.17315036076248214\n",
      "iter: 50, loss: 0.6357649872581045, w_module: 0.23222013767348068\n",
      "iter: 60, loss: 0.5934601510018419, w_module: 0.2899487967230823\n",
      "iter: 70, loss: 0.5548934089119313, w_module: 0.34592273927558675\n",
      "iter: 80, loss: 0.5197714261467372, w_module: 0.4000002565589039\n",
      "iter: 90, loss: 0.4878003944221882, w_module: 0.45213706287942257\n",
      "iter: 100, loss: 0.4586946072129149, w_module: 0.5023380305862867\n",
      "iter: 110, loss: 0.43218247975386254, w_module: 0.5506373613810752\n",
      "iter: 120, loss: 0.40801035161278965, w_module: 0.5970879354275748\n",
      "iter: 130, loss: 0.3859445066923714, w_module: 0.6417545853153225\n",
      "iter: 140, loss: 0.36577185014318214, w_module: 0.6847094569326769\n",
      "iter: 150, loss: 0.3472996344787591, w_module: 0.7260286759574365\n",
      "iter: 160, loss: 0.3303545575631868, w_module: 0.7657899223393912\n",
      "iter: 170, loss: 0.3147814820279409, w_module: 0.804070673858469\n",
      "iter: 180, loss: 0.30044195943406254, w_module: 0.8409469555718961\n",
      "iter: 190, loss: 0.28721268751668005, w_module: 0.8764924745296478\n",
      "iter: 200, loss: 0.27498398590157924, w_module: 0.9107780470536629\n",
      "iter: 210, loss: 0.2636583436641247, w_module: 0.9438712463021535\n",
      "iter: 220, loss: 0.2531490691528017, w_module: 0.9758362136902946\n",
      "iter: 230, loss: 0.24337905665003737, w_module: 1.0067335903283532\n",
      "iter: 240, loss: 0.23427967391332252, w_module: 1.0366205346812312\n",
      "iter: 250, loss: 0.2257897679623629, w_module: 1.0655508006290018\n",
      "iter: 260, loss: 0.21785478250803827, w_module: 1.093574856392494\n",
      "iter: 270, loss: 0.21042597829805543, w_module: 1.120740029696839\n",
      "iter: 280, loss: 0.20345974675828554, w_module: 1.147090668347306\n",
      "iter: 290, loss: 0.19691700719555263, w_module: 1.1726683083114686\n",
      "iter: 300, loss: 0.19076267819061707, w_module: 1.1975118436266725\n",
      "iter: 310, loss: 0.18496521444165373, w_module: 1.2216576941343258\n",
      "iter: 320, loss: 0.17949620108139158, w_module: 1.2451399683049635\n",
      "iter: 330, loss: 0.17432999829710955, w_module: 1.2679906193574253\n",
      "iter: 340, loss: 0.16944342987742472, w_module: 1.2902395935679611\n",
      "iter: 350, loss: 0.1648155100615328, w_module: 1.3119149701699864\n",
      "iter: 360, loss: 0.16042720375852373, w_module: 1.3330430926083372\n",
      "iter: 370, loss: 0.1562612158295393, w_module: 1.3536486911685601\n",
      "iter: 380, loss: 0.1523018056828423, w_module: 1.3737549971788527\n",
      "iter: 390, loss: 0.14853462392390676, w_module: 1.3933838491003834\n",
      "iter: 400, loss: 0.14494656823399069, w_module: 1.4125557908963462\n",
      "iter: 410, loss: 0.1415256560268352, w_module: 1.431290163113057\n",
      "iter: 420, loss: 0.13826091175995736, w_module: 1.4496051871264983\n",
      "iter: 430, loss: 0.13514226706017346, w_module: 1.4675180430116073\n",
      "iter: 440, loss: 0.1321604720678674, w_module: 1.4850449414842106\n",
      "iter: 450, loss: 0.12930701661602986, w_module: 1.502201190350541\n",
      "iter: 460, loss: 0.1265740600426438, w_module: 1.519001255879352\n",
      "iter: 470, loss: 0.12395436859250814, w_module: 1.5354588194888565\n",
      "iter: 480, loss: 0.12144125950051032, w_module: 1.5515868301163669\n",
      "iter: 490, loss: 0.11902855096567105, w_module: 1.5673975526137214\n",
      "iter: 500, loss: 0.11671051732660624, w_module: 1.5829026124869934\n",
      "iter: 510, loss: 0.11448184883660721, w_module: 1.598113037275089\n",
      "iter: 520, loss: 0.11233761551228091, w_module: 1.6130392948390209\n",
      "iter: 530, loss: 0.11027323459526493, w_module: 1.6276913288119914\n",
      "iter: 540, loss: 0.1082844412233709, w_module: 1.6420785914401317\n",
      "iter: 550, loss: 0.10636726195682852, w_module: 1.6562100740248151\n",
      "iter: 560, loss: 0.10451799084815354, w_module: 1.6700943351599065\n",
      "iter: 570, loss: 0.10273316778143712, w_module: 1.6837395269410667\n",
      "iter: 580, loss: 0.10100955883932193, w_module: 1.697153419309313\n",
      "iter: 590, loss: 0.09934413848424735, w_module: 1.7103434226772802\n",
      "iter: 600, loss: 0.09773407336527852, w_module: 1.7233166089740675\n",
      "iter: 610, loss: 0.09617670758346318, w_module: 1.736079731233001\n",
      "iter: 620, loss: 0.09466954926760082, w_module: 1.7486392418361443\n",
      "iter: 630, loss: 0.0932102583289218, w_module: 1.761001309519732\n",
      "iter: 640, loss: 0.09179663527775944, w_module: 1.7731718352359482\n",
      "iter: 650, loss: 0.09042661099812697, w_module: 1.7851564669584248\n",
      "iter: 660, loss: 0.08909823738740719, w_module: 1.7969606135115619\n",
      "iter: 670, loss: 0.08780967877832102, w_module: 1.8085894574970738\n",
      "iter: 680, loss: 0.08655920406913825, w_module: 1.820047967385109\n",
      "iter: 690, loss: 0.08534517949586619, w_module: 1.8313409088317347\n",
      "iter: 700, loss: 0.0841660619870393, w_module: 1.8424728552795264\n",
      "iter: 710, loss: 0.08302039304783065, w_module: 1.8534481978933899\n",
      "iter: 720, loss: 0.08190679312562592, w_module: 1.8642711548795337\n",
      "iter: 730, loss: 0.08082395641401349, w_module: 1.874945780231664\n",
      "iter: 740, loss: 0.07977064605642559, w_module: 1.8854759719449647\n",
      "iter: 750, loss: 0.07874568971448588, w_module: 1.8958654797352261\n",
      "iter: 760, loss: 0.07774797546951957, w_module: 1.906117912297539\n",
      "iter: 770, loss: 0.07677644802872552, w_module: 1.916236744136309\n",
      "iter: 780, loss: 0.07583010521022802, w_module: 1.926225321995875\n",
      "iter: 790, loss: 0.07490799468365958, w_module: 1.9360868709187837\n",
      "iter: 800, loss: 0.07400921094510962, w_module: 1.9458244999567036\n",
      "iter: 810, loss: 0.07313289250722918, w_module: 1.9554412075570824\n",
      "iter: 820, loss: 0.07227821928704262, w_module: 1.964939886646924\n",
      "iter: 830, loss: 0.07144441017559819, w_module: 1.9743233294334637\n",
      "iter: 840, loss: 0.07063072077501212, w_module: 1.983594231940075\n",
      "iter: 850, loss: 0.06983644128974596, w_module: 1.992755198294392\n",
      "iter: 860, loss: 0.06906089456011268, w_module: 2.0018087447844097\n",
      "iter: 870, loss: 0.06830343422705408, w_module: 2.010757303697181\n",
      "iter: 880, loss: 0.06756344301817607, w_module: 2.019603226953691\n",
      "iter: 890, loss: 0.0668403311458845, w_module: 2.0283487895525383\n",
      "iter: 900, loss: 0.06613353480923835, w_module: 2.036996192834149\n",
      "iter: 910, loss: 0.06544251479184007, w_module: 2.0455475675764427\n",
      "iter: 920, loss: 0.06476675514871963, w_module: 2.0540049769321267\n",
      "iter: 930, loss: 0.06410576197574883, w_module: 2.062370419217072\n",
      "iter: 940, loss: 0.06345906225564787, w_module: 2.070645830558612\n",
      "iter: 950, loss: 0.06282620277512704, w_module: 2.078833087411985\n",
      "iter: 960, loss: 0.06220674910814049, w_module: 2.086934008952611\n",
      "iter: 970, loss: 0.061600284660628925, w_module: 2.094950359351369\n",
      "iter: 980, loss: 0.06100640977249032, w_module: 2.1028838499395843\n",
      "iter: 990, loss: 0.06042474087284946, w_module: 2.11073614126998\n",
      "[-1.50843471 -1.48641405  5.50347691]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# create a LogisticRegression object and train it when using regularization\n",
    "from logistic import LogisticRegression\n",
    "\n",
    "LR = LogisticRegression()\n",
    "LR.fit(x_train, y_train, lr=0.1, n_iter=1000,reg=0.1)\n",
    "# LR.fit(x_train, y_train, lr=0.1, n_iter=1000,reg=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 100.0\n",
      "Test accuracy: 97.0\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement the code to compute the accuracy of logistic regression (LR) in the test set. Note that LR itself is already trained, if you have run the above code.\n",
    "_, train_pred = LR.predict(x_train)\n",
    "train_acc = np.mean(train_pred == y_train) * 100\n",
    "\n",
    "print(\"Train accuracy: {}\".format(train_acc))\n",
    "\n",
    "_, test_pred = LR.predict(x_test)\n",
    "test_acc = np.mean(test_pred == y_test) * 100\n",
    "\n",
    "print(\"Test accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上述带有正则化的代码后，请观察 $||w||$ 的变化，并讨论正则化的实际意义。(请将答案写在下方)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||w||变小了，可见正则化可以限制模型的复杂度，从而防止过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
